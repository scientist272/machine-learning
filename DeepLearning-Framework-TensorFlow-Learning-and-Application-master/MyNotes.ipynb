{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《深度学习框架Tensorflow学习与应用》笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一课\n",
    "这节课主要介绍了下TensorFlow、Annacoda的安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二课"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建图、启动图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_2:0\", shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#创建一个常量op\n",
    "m1 = tf.constant([[3,3]])\n",
    "## 创建一个常量op\n",
    "m2 = tf.constant([[2],[3]])\n",
    "#创建一个矩阵乘法op，吧m1和 m2 传入\n",
    "product = tf.matmul(m1,m2)\n",
    "print(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：这里只是定义了一些operation，并没有放到会话里边的图中进行执行！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15]]\n"
     ]
    }
   ],
   "source": [
    "# 定义了一个会话，启动默认图\n",
    "sess = tf.Session()\n",
    "## 调用session的 run 方法来执行矩阵乘法op\n",
    "## run(product)触发了图中的3个op\n",
    "result = sess.run(product)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样，会话才开始执行，首先执行矩阵乘法，然后再上一层生成两个常量，它就是一层一层的往上调用！  \n",
    "最后要把会话关闭！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 结束时会自动关闭session\n",
    "    result = sess.run(product)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用了这个with语句，我们就不需要关闭会话了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2 -1]\n",
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([1,2])\n",
    "a = tf.constant([3,3])\n",
    "\n",
    "sub = tf.subtract(x,a)\n",
    "add = tf.add(x,sub)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(sub))\n",
    "    print(sess.run(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：变量Variable()需要初始化！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**变量的自加：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "## 创建一个变量初始为0\n",
    "state = tf.Variable(0,name='counter')\n",
    "## 创建一个op，作用是使state+1\n",
    "new_state = tf.add(state,1)\n",
    "## 赋值op\n",
    "update = tf.assign(state,new_state)\n",
    "## 变量初始化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(state))\n",
    "    for _ in range(5):\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch&Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as  tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch: 我们可以在会话中同时运行多个op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "## fetch\n",
    "input1 =tf.constant(3.0)\n",
    "input2= tf.constant(2.0)\n",
    "input3 = tf.constant(5.0)\n",
    "\n",
    "add = tf.add(input2,input3)\n",
    "mul = tf.multiply(input1,add)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run([mul,add])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.]\n"
     ]
    }
   ],
   "source": [
    "# Feed\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = tf.multiply(input1,input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ## feed 的数据以字典的形式传入\n",
    "    print(sess.run(output,feed_dict={input1:[8.],input2:[2.]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow简单演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.060734015, 0.102564335]\n",
      "20 [0.10898718, 0.19464187]\n",
      "40 [0.105815746, 0.1965327]\n",
      "60 [0.10376346, 0.19775626]\n",
      "80 [0.10243539, 0.19854805]\n",
      "100 [0.101575986, 0.19906041]\n",
      "120 [0.10101985, 0.19939198]\n",
      "140 [0.10065995, 0.19960654]\n",
      "160 [0.100427054, 0.19974539]\n",
      "180 [0.10027636, 0.19983524]\n",
      "200 [0.10017883, 0.19989339]\n"
     ]
    }
   ],
   "source": [
    "## 使用numpy生成100个随机点\n",
    "x_data = np.random.rand(100)\n",
    "y_data = x_data*0.1+0.2\n",
    "\n",
    "#构建一个线性模型\n",
    "b = tf.Variable(0.)\n",
    "k = tf.Variable(0.)\n",
    "y = k*x_data+b\n",
    "## 损失函数\n",
    "loss = tf.reduce_mean(tf.square(y_data-y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.2)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(201):\n",
    "        sess.run(train)\n",
    "        if step %20 == 0:\n",
    "            print(step,sess.run([k,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三课"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归（这里其实是非线性回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21079384828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 使用numpy生成200个随机点\n",
    "x_data = np.linspace(-0.5,0.5,200)[:,np.newaxis]\n",
    "noise =np.random.normal(0,0.02,x_data.shape)\n",
    "y_data = np.square(x_data) + noise\n",
    "\n",
    "## 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32,[None,1])\n",
    "y = tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "## 定义神经网络中间层\n",
    "Weights_L1 = tf.Variable(tf.random_normal([1,10]))\n",
    "biases_L1 = tf.Variable(tf.zeros([1,10]))\n",
    "Wx_plus_b_L1 = tf.matmul(x,Weights_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1)\n",
    "\n",
    "## 定义神经网络输出层\n",
    "Weights_L2 = tf.Variable(tf.random_normal([10,1]))\n",
    "biases_L2 = tf.Variable(tf.zeros([1,1]))\n",
    "Wx_plus_b_L2 = tf.matmul(L1,Weights_L2) + biases_L2\n",
    "prediction = tf.nn.tanh(Wx_plus_b_L2)\n",
    "\n",
    "## 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "## 使用梯度下降法训练\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(2000):\n",
    "        sess.run(train_step,feed_dict={x:x_data,y:y_data})\n",
    "        \n",
    "    prediction_value = sess.run(prediction,feed_dict={x:x_data})\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(x_data,y_data)\n",
    "    plt.plot(x_data,prediction_value,'r-',lw=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST数据分类简单版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "## 每个批次的大小\n",
    "batch_size =100\n",
    "## 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "## 定义两个placeholder\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "## 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "\n",
    "## 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "## 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "## 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "## 结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) ## argmax 返回一维张量中最大的值所在的位置\n",
    "## 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print (\"Iter\" + str(epoch)+\" ,Testing Accuracy\" +str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的tf.argmax()返回最大值所在的位置（索引），因为这里y、prediction都是one-hot标签（y为9个0，一个1；prediction为10个0-1之间的概率），所以返回的就是真实值和预测值的索引，如果一样说明预测正确！  \n",
    "注意：返回得到的值为布尔型的列表，这里需要把布尔型的列表转化为浮点型，然后求平均值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四课"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 11001] getaddrinfo failed>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[1;32m-> 1318\u001b[1;33m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[0;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1233\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m         self.sock = self._create_connection(\n\u001b[1;32m--> 936\u001b[1;33m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[0;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-09548f4066c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## 载入数据集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MNIST_data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m## 每个批次的大小\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m## 计算一共有多少个批次\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m               instructions)\n\u001b[1;32m--> 250\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    252\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py\u001b[0m in \u001b[0;36mread_data_sets\u001b[1;34m(train_dir, fake_data, one_hot, dtype, reshape, validation_size, seed, source_url)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m   local_file = base.maybe_download(TRAIN_IMAGES, train_dir,\n\u001b[1;32m--> 260\u001b[1;33m                                    source_url + TRAIN_IMAGES)\n\u001b[0m\u001b[0;32m    261\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m               instructions)\n\u001b[1;32m--> 250\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    252\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\u001b[0m in \u001b[0;36mmaybe_download\u001b[1;34m(filename, work_directory, source_url)\u001b[0m\n\u001b[0;32m    250\u001b[0m   \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[0mtemp_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlretrieve_with_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m     \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m               instructions)\n\u001b[1;32m--> 250\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    252\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mdelay\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdelays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mis_retriable\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\u001b[0m in \u001b[0;36murlretrieve_with_retry\u001b[1;34m(url, filename)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_internal_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_retriable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_is_retriable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0murlretrieve_with_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 544\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[1;32m-> 1361\u001b[1;33m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1318\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [Errno 11001] getaddrinfo failed>"
     ]
    }
   ],
   "source": [
    "## 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "## 每个批次的大小\n",
    "batch_size =100\n",
    "## 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "## 定义两个placeholder\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "## 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "\n",
    "## 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "## 交叉熵\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = prediction))\n",
    "## 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "## 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "## 结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) ## argmax 返回一维张量中最大的值所在的位置\n",
    "## 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print (\"Iter\" + str(epoch)+\" ,Testing Accuracy\" +str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：交叉熵这里需要求一个平均值**  \n",
    "\n",
    "损失函数采用交叉熵将会加快训练速度（相比于二次代价函数）,这里只迭代了3次就达到了90%的正确率，而二次代价函数这里需要迭代7次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Iter0 ,Testing Accuracy0.9331 ,Training Accuracy0.9335273\n",
      "Iter1 ,Testing Accuracy0.9495 ,Training Accuracy0.9535273\n",
      "Iter2 ,Testing Accuracy0.9568 ,Training Accuracy0.96265453\n",
      "Iter3 ,Testing Accuracy0.9608 ,Training Accuracy0.9666\n",
      "Iter4 ,Testing Accuracy0.9651 ,Training Accuracy0.9729818\n",
      "Iter5 ,Testing Accuracy0.9673 ,Training Accuracy0.97587276\n",
      "Iter6 ,Testing Accuracy0.9706 ,Training Accuracy0.97876364\n",
      "Iter7 ,Testing Accuracy0.9699 ,Training Accuracy0.9810909\n",
      "Iter8 ,Testing Accuracy0.9708 ,Training Accuracy0.9837273\n",
      "Iter9 ,Testing Accuracy0.9712 ,Training Accuracy0.9844364\n",
      "Iter10 ,Testing Accuracy0.9732 ,Training Accuracy0.98643637\n",
      "Iter11 ,Testing Accuracy0.9766 ,Training Accuracy0.98809093\n",
      "Iter12 ,Testing Accuracy0.9755 ,Training Accuracy0.9885273\n",
      "Iter13 ,Testing Accuracy0.976 ,Training Accuracy0.9892\n",
      "Iter14 ,Testing Accuracy0.9767 ,Training Accuracy0.9901091\n",
      "Iter15 ,Testing Accuracy0.9764 ,Training Accuracy0.9906545\n",
      "Iter16 ,Testing Accuracy0.9767 ,Training Accuracy0.9917455\n",
      "Iter17 ,Testing Accuracy0.9776 ,Training Accuracy0.9922\n",
      "Iter18 ,Testing Accuracy0.9761 ,Training Accuracy0.99258184\n",
      "Iter19 ,Testing Accuracy0.9776 ,Training Accuracy0.9926364\n",
      "Iter20 ,Testing Accuracy0.9763 ,Training Accuracy0.99307275\n"
     ]
    }
   ],
   "source": [
    "## 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "## 每个批次的大小\n",
    "batch_size =100\n",
    "## 计算一共有多少个批次\n",
    "\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "## 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "## 创建一个简单的神经网络\n",
    "W1 = tf.Variable(tf.truncated_normal([784,200],stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([200])+0.1)\n",
    "L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)\n",
    "L1_drop = tf.nn.dropout(L1,keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([200,200],stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([200])+0.1)\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)\n",
    "L2_drop = tf.nn.dropout(L2,keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([200,100],stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([100])+0.1)\n",
    "L3 = tf.nn.tanh(tf.matmul(L2_drop,W3)+b3)\n",
    "L3_drop = tf.nn.dropout(L3,keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([100,10],stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10])+0.1)\n",
    "prediction = tf.nn.tanh(tf.matmul(L3_drop,W4)+b4)\n",
    "\n",
    "\n",
    "## 交叉熵\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = prediction))\n",
    "## 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "## 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "## 结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) ## argmax 返回一维张量中最大的值所在的位置\n",
    "## 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            ## 注意 只有训练的时候能 进行dropOut,测试时候不能使用dropout\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "        \n",
    "        ## 下面是 测试, keep_prob 必须为1\n",
    "        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0})\n",
    "        print (\"Iter\" + str(epoch)+\" ,Testing Accuracy\" +str(test_acc)+\" ,Training Accuracy\" +str(train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题1\n",
    "**注意：这里初始化和之前不同，因为多层神经网络采用全0初始化会有问题，所以采用tf.truncated_normal()函数（截断的正态分布中输出随机值）**  \n",
    "\n",
    "tf.truncated_normal(): 生成的值服从具有指定平均值和标准偏差的正态分布，如果生成的值大于平均值2个标准偏差的值则丢弃重新选择。\n",
    "\n",
    "#### 问题2\n",
    "这里故意选了很多层的神经网络，为了过拟合，这样之后可以看到dropout的作用  \n",
    "- 第一次（keep_prop = 1.0）：没有dropout，可以看出训练集的正确率达到99.5%，测试集的正确率达到97%  \n",
    "- 第二次（keep_prop = 0.7）：训练时70%的神经元工作，**测试时所有的神经元工作！！**这里可以看到模型的收敛速度变慢了，同时训练集、测试集的正确率基本一致，说明没有过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练速度\n",
    "SGD优化器训练的速度慢，Momentum、NAG（“聪明”的Momentum）优化的路径会有错误，但是速度很快\n",
    "\n",
    "#### 鞍点问题\n",
    "SGD优化器没有办法从鞍点逃离，别的优化器都可以逃离\n",
    "\n",
    "但是训练速度并不是衡量优劣的标准，一般来说准确率才是关键，所以SGD优化器还是继续在使用，在创建一个新的神经网络的时候，可以采用速度最快的优化器进行训练，然后训练了差不多的时候，把所有的优化器都试一遍，找到准确率最高的优化器。\n",
    "\n",
    "#### 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter0 ,Testing Accuracy0.6533\n",
      "Iter1 ,Testing Accuracy0.6604\n",
      "Iter2 ,Testing Accuracy0.6662\n",
      "Iter3 ,Testing Accuracy0.6718\n",
      "Iter4 ,Testing Accuracy0.6756\n",
      "Iter5 ,Testing Accuracy0.678\n",
      "Iter6 ,Testing Accuracy0.6805\n",
      "Iter7 ,Testing Accuracy0.6829\n",
      "Iter8 ,Testing Accuracy0.6854\n",
      "Iter9 ,Testing Accuracy0.6895\n",
      "Iter10 ,Testing Accuracy0.6913\n",
      "Iter11 ,Testing Accuracy0.6915\n",
      "Iter12 ,Testing Accuracy0.6925\n",
      "Iter13 ,Testing Accuracy0.695\n",
      "Iter14 ,Testing Accuracy0.6966\n",
      "Iter15 ,Testing Accuracy0.6977\n",
      "Iter16 ,Testing Accuracy0.6987\n",
      "Iter17 ,Testing Accuracy0.7\n",
      "Iter18 ,Testing Accuracy0.7018\n",
      "Iter19 ,Testing Accuracy0.7039\n",
      "Iter20 ,Testing Accuracy0.7056\n"
     ]
    }
   ],
   "source": [
    "## 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "## 每个批次的大小\n",
    "batch_size =100\n",
    "## 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "## 定义两个placeholder\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "## 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "\n",
    "## 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "## 使用梯度下降法\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "## 一般在使用 AdamOptimizer 的learning rate 都比较小,1e-5,1e-6\n",
    "train_step = tf.train.AdamOptimizer(1e-6).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "## 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "## 结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) ## argmax 返回一维张量中最大的值所在的位置\n",
    "## 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print (\"Iter\" + str(epoch)+\" ,Testing Accuracy\" +str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般Adam优化器的学习率都比较小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第四周作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter0 ,Testing Accuracy0.9582learning rate =0.001\n",
      "Iter1 ,Testing Accuracy0.9641learning rate =0.00095\n",
      "Iter2 ,Testing Accuracy0.9711learning rate =0.0009025\n",
      "Iter3 ,Testing Accuracy0.9756learning rate =0.000857375\n",
      "Iter4 ,Testing Accuracy0.977learning rate =0.00081450626\n",
      "Iter5 ,Testing Accuracy0.9784learning rate =0.0007737809\n",
      "Iter6 ,Testing Accuracy0.9764learning rate =0.0007350919\n",
      "Iter7 ,Testing Accuracy0.9773learning rate =0.0006983373\n",
      "Iter8 ,Testing Accuracy0.9794learning rate =0.0006634204\n",
      "Iter9 ,Testing Accuracy0.9781learning rate =0.0006302494\n",
      "Iter10 ,Testing Accuracy0.9803learning rate =0.0005987369\n",
      "Iter11 ,Testing Accuracy0.9813learning rate =0.0005688001\n",
      "Iter12 ,Testing Accuracy0.9809learning rate =0.0005403601\n",
      "Iter13 ,Testing Accuracy0.9798learning rate =0.0005133421\n",
      "Iter14 ,Testing Accuracy0.9817learning rate =0.000487675\n",
      "Iter15 ,Testing Accuracy0.9802learning rate =0.00046329122\n",
      "Iter16 ,Testing Accuracy0.9814learning rate =0.00044012666\n",
      "Iter17 ,Testing Accuracy0.9816learning rate =0.00041812033\n",
      "Iter18 ,Testing Accuracy0.9821learning rate =0.00039721432\n",
      "Iter19 ,Testing Accuracy0.9831learning rate =0.0003773536\n",
      "Iter20 ,Testing Accuracy0.9803learning rate =0.00035848594\n",
      "Iter21 ,Testing Accuracy0.9815learning rate =0.00034056162\n",
      "Iter22 ,Testing Accuracy0.9827learning rate =0.00032353355\n",
      "Iter23 ,Testing Accuracy0.9823learning rate =0.00030735688\n",
      "Iter24 ,Testing Accuracy0.9829learning rate =0.000291989\n",
      "Iter25 ,Testing Accuracy0.9834learning rate =0.00027738957\n",
      "Iter26 ,Testing Accuracy0.9832learning rate =0.0002635201\n",
      "Iter27 ,Testing Accuracy0.9826learning rate =0.00025034408\n",
      "Iter28 ,Testing Accuracy0.9824learning rate =0.00023782688\n",
      "Iter29 ,Testing Accuracy0.9825learning rate =0.00022593554\n",
      "Iter30 ,Testing Accuracy0.9825learning rate =0.00021463877\n",
      "Iter31 ,Testing Accuracy0.9825learning rate =0.00020390682\n",
      "Iter32 ,Testing Accuracy0.9827learning rate =0.00019371149\n",
      "Iter33 ,Testing Accuracy0.9829learning rate =0.0001840259\n",
      "Iter34 ,Testing Accuracy0.9831learning rate =0.00017482461\n",
      "Iter35 ,Testing Accuracy0.9823learning rate =0.00016608338\n",
      "Iter36 ,Testing Accuracy0.9825learning rate =0.00015777921\n",
      "Iter37 ,Testing Accuracy0.9826learning rate =0.00014989026\n",
      "Iter38 ,Testing Accuracy0.9827learning rate =0.00014239574\n",
      "Iter39 ,Testing Accuracy0.9828learning rate =0.00013527596\n",
      "Iter40 ,Testing Accuracy0.9827learning rate =0.00012851215\n",
      "Iter41 ,Testing Accuracy0.9823learning rate =0.00012208655\n",
      "Iter42 ,Testing Accuracy0.9833learning rate =0.00011598222\n",
      "Iter43 ,Testing Accuracy0.9824learning rate =0.00011018311\n",
      "Iter44 ,Testing Accuracy0.9824learning rate =0.000104673956\n",
      "Iter45 ,Testing Accuracy0.9831learning rate =9.944026e-05\n",
      "Iter46 ,Testing Accuracy0.983learning rate =9.446825e-05\n",
      "Iter47 ,Testing Accuracy0.9831learning rate =8.974483e-05\n",
      "Iter48 ,Testing Accuracy0.9827learning rate =8.525759e-05\n",
      "Iter49 ,Testing Accuracy0.982learning rate =8.099471e-05\n",
      "Iter50 ,Testing Accuracy0.9828learning rate =7.6944976e-05\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "## 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "## 每个批次的大小\n",
    "batch_size =100\n",
    "## 计算一共有多少个批次\n",
    "\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "## 定义两个placeholder\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001,dtype=tf.float32)\n",
    "\n",
    "## 创建一个简单的神经网络\n",
    "W1 = tf.Variable(tf.truncated_normal([784,500],stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([500])+0.1)\n",
    "L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)\n",
    "L1_drop = tf.nn.dropout(L1,keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([500,300],stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([300])+0.1)\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)\n",
    "L2_drop = tf.nn.dropout(L2,keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([300,10],stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10])+0.1)\n",
    "prediction = tf.nn.tanh(tf.matmul(L2_drop,W3)+b3)\n",
    "\n",
    "\n",
    "## 交叉熵\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) ## argmax 返回一维张量中最大的值所在的位置\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(51):\n",
    "        ## 使得学习率随着收敛逐步缩小,刚开始很大,快速收敛,靠近最小值时,使得慢慢靠近,使得到达最低点\n",
    "        sess.run(tf.assign(lr,0.001*(0.95**epoch)))\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "        \n",
    "        learning_rate = sess.run(lr)\n",
    "        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        print (\"Iter\" + str(epoch)+\" ,Testing Accuracy\" +str(test_acc)+\"learning rate =\"+str(learning_rate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 因为手写数字识别数据集的数据并不多，所以这里不使用dropout。  \n",
    "- 采用Adam优化器加快收敛  \n",
    "- 采用合适的epoch次数，这里刚开始是比较大的学习率，然后慢慢降低学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五课\n",
    "TensorBoard使用教程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard网络结构\n",
    "由于这里看的是网络的结构，所以只需要迭代一次就可以\n",
    "\n",
    "#### 保存路径\n",
    "writer = tf.summary.FileWriter('logs/',sess.graph)  \n",
    "注意盘符，必须要移动到跟文件所在位置一样的盘符\n",
    "\n",
    "**问题：在浏览器输入这个链接(localhost:6006)的时候却不能找到程序生成的数据流图，而我在文件系统下在该文件夹里却能正常找到文件** \n",
    "\n",
    "解决办法：一路cd到训练数据所在文件夹，如果在其他磁盘，先转到所在盘，再cd到所在文件夹，然后打开cmd，输入：**tensorboard --logdir = ./**\n",
    "\n",
    "#### graph的缓存清除\n",
    "每次执行之后，graph都会保存在内存中，可以通过restart kernel来解决。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-1-2317c4a9fcd0>:27: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Iter0 ,Testing Accuracy0.8242\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "\n",
    "batch_size =100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "## 命名空间 \n",
    "with tf.name_scope(\"input\"):\n",
    "    x=tf.placeholder(tf.float32,[None,784],name=\"x-input\")\n",
    "    y = tf.placeholder(tf.float32,[None,10],name=\"y-input\")\n",
    "\n",
    "with tf.name_scope('layer'):\n",
    "    with tf.name_scope('weights'):\n",
    "        W = tf.Variable(tf.zeros([784,10]))\n",
    "    with tf.name_scope('biases'):\n",
    "        b = tf.Variable(tf.zeros([10]))\n",
    "    with tf.name_scope('wx_plus_b'):\n",
    "        wx_plus_b = tf.matmul(x,W)+b\n",
    "    with tf.name_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(wx_plus_b)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) ## argmax 返回一维张量中最大的值所在的位置\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "## 下面和网络结构没有关系,只有上面的和 网络结构有关系\n",
    "## 注意重新启动 jupyter 以及 删除 原来的log 目录\n",
    "## windows 输入 logdir要进入相应的磁盘 ,输入全路径\\\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter('logs/',sess.graph)\n",
    "    for epoch in range(1):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print (\"Iter\" + str(epoch)+\" ,Testing Accuracy\" +str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard网络运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数概要\n",
    "tf.summary.scalar('mean',mean)\n",
    "\n",
    "#### 合并所有的summary\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "#### 写入文件\n",
    "writer.add_summary(summary,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter0 ,Testing Accuracy0.8479\n",
      "Iter1 ,Testing Accuracy0.8969\n",
      "Iter2 ,Testing Accuracy0.902\n",
      "Iter3 ,Testing Accuracy0.9059\n",
      "Iter4 ,Testing Accuracy0.9082\n",
      "Iter5 ,Testing Accuracy0.9096\n",
      "Iter6 ,Testing Accuracy0.9124\n",
      "Iter7 ,Testing Accuracy0.9139\n",
      "Iter8 ,Testing Accuracy0.9145\n",
      "Iter9 ,Testing Accuracy0.9164\n",
      "Iter10 ,Testing Accuracy0.9167\n",
      "Iter11 ,Testing Accuracy0.9178\n",
      "Iter12 ,Testing Accuracy0.9192\n",
      "Iter13 ,Testing Accuracy0.9192\n",
      "Iter14 ,Testing Accuracy0.9206\n",
      "Iter15 ,Testing Accuracy0.92\n",
      "Iter16 ,Testing Accuracy0.9205\n",
      "Iter17 ,Testing Accuracy0.9206\n",
      "Iter18 ,Testing Accuracy0.9216\n",
      "Iter19 ,Testing Accuracy0.9211\n",
      "Iter20 ,Testing Accuracy0.9217\n",
      "Iter21 ,Testing Accuracy0.9216\n",
      "Iter22 ,Testing Accuracy0.9224\n",
      "Iter23 ,Testing Accuracy0.9216\n",
      "Iter24 ,Testing Accuracy0.9224\n",
      "Iter25 ,Testing Accuracy0.923\n",
      "Iter26 ,Testing Accuracy0.9231\n",
      "Iter27 ,Testing Accuracy0.9231\n",
      "Iter28 ,Testing Accuracy0.9243\n",
      "Iter29 ,Testing Accuracy0.9239\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "batch_size =100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "##参数概要\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean) # 平均值\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histgram',var) # 直方图\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    x=tf.placeholder(tf.float32,[None,784],name=\"x-input\")\n",
    "    y = tf.placeholder(tf.float32,[None,10],name=\"y-input\")\n",
    "\n",
    "with tf.name_scope('layer'):\n",
    "    with tf.name_scope('weights'):\n",
    "        W = tf.Variable(tf.zeros([784,10]))\n",
    "        variable_summaries(W) # 分析w\n",
    "    with tf.name_scope('biases'):\n",
    "        b = tf.Variable(tf.zeros([10]))\n",
    "        variable_summaries(b) # 分析b\n",
    "    with tf.name_scope('wx_plus_b'):\n",
    "        wx_plus_b = tf.matmul(x,W)+b\n",
    "    with tf.name_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(wx_plus_b)\n",
    "\n",
    "## 如果loss 曲线震荡太大,可能学习率设置太大了\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=prediction))\n",
    "    tf.summary.scalar('loss',loss)\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) ## argmax 返回一维张量中最大的值所在的位置\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "        \n",
    "## 合并所有的 summary\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "## 下面和网络结构没有关系,只有上面的和 网络结构有关系\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter('logs/',sess.graph)\n",
    "    for epoch in range(30):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            summary,_=sess.run([merged,train_step],feed_dict={x:batch_xs,y:batch_ys})\n",
    "        \n",
    "        ## 所有样本运行一个周期 之后再次打个点    \n",
    "        writer.add_summary(summary,epoch)\n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print (\"Iter\" + str(epoch)+\" ,Testing Accuracy\" +str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard结果可以看出：  \n",
    "\n",
    "1.scaler  \n",
    "- 正确率开始慢慢增加，然后基本不再增加  \n",
    "- weights、biases的最小值、最大值、平均值的变化\n",
    "- loss在慢慢减小，如果没有很大的振荡就说明学习率设置的还可以，没有不收敛的现象  \n",
    "\n",
    "![5-01](https://github.com/dta0502/DeepLearning-Framework-TensorFlow-Learning-and-Application/blob/master/Notebook%20images/5-01.PNG)\n",
    "\n",
    "2.distribution  \n",
    "weights、biases的分布，颜色越深、分布越多  \n",
    "\n",
    "![5-02](https://github.com/dta0502/DeepLearning-Framework-TensorFlow-Learning-and-Application/blob/master/Notebook%20images/5-02.PNG)\n",
    "\n",
    "3.histogram  \n",
    "weights、biases的分布随着训练次数的变化而变化  \n",
    "\n",
    "![5-03](https://github.com/dta0502/DeepLearning-Framework-TensorFlow-Learning-and-Application/blob/master/Notebook%20images/5-03.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard可视化\n",
    "\n",
    "#### TensorFlow官网上的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter0 ,Testing Accuracy0.1413\n",
      "Iter100 ,Testing Accuracy0.7128\n",
      "Iter200 ,Testing Accuracy0.7337\n",
      "Iter300 ,Testing Accuracy0.7426\n",
      "Iter400 ,Testing Accuracy0.7686\n",
      "Iter500 ,Testing Accuracy0.8223\n",
      "Iter600 ,Testing Accuracy0.8272\n",
      "Iter700 ,Testing Accuracy0.8574\n",
      "Iter800 ,Testing Accuracy0.8732\n",
      "Iter900 ,Testing Accuracy0.8855\n",
      "Iter1000 ,Testing Accuracy0.8899\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data  \n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "# 运行次数\n",
    "max_steps = 1001\n",
    "## 图片数量\n",
    "image_num = 3000\n",
    "#文件路进\n",
    "DIR ='D://document/Dropbox/document/Python/DeepLearning Framework TensorFlow Learning and Application/'\n",
    "# 定义绘会话\n",
    "sess = tf.Session()\n",
    "\n",
    "#载入图片\n",
    "embedding = tf.Variable(tf.stack(mnist.test.images[:image_num]),trainable=False,name='embedding')\n",
    "\n",
    "\n",
    "##参数概要\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean) # 平均值\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histgram',var) # 直方图\n",
    "\n",
    "## 命名空间\n",
    "with tf.name_scope(\"input\"):\n",
    "    x=tf.placeholder(tf.float32,[None,784],name=\"x-input\")\n",
    "    y = tf.placeholder(tf.float32,[None,10],name=\"y-input\")\n",
    "\n",
    "\n",
    "# 显示图皮那\n",
    "with tf.name_scope(\"input_reshape\"):\n",
    "    # -1 表示任意值,原来一张图片是一行1x784,现在百纳成28x28,channel=1因为是黑白图片\n",
    "    image_shaped_input = tf.reshape(x,[-1,28,28,1])\n",
    "    tf.summary.image(\"input\",image_shaped_input,10)\n",
    "\n",
    "with tf.name_scope('layer'):\n",
    "    with tf.name_scope('weights'):\n",
    "        W = tf.Variable(tf.zeros([784,10]))\n",
    "        variable_summaries(W) ## 分析w\n",
    "    with tf.name_scope('biases'):\n",
    "        b = tf.Variable(tf.zeros([10]))\n",
    "        variable_summaries(b) # 分析b\n",
    "    with tf.name_scope('wx_plus_b'):\n",
    "        wx_plus_b = tf.matmul(x,W)+b\n",
    "    with tf.name_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(wx_plus_b)\n",
    "    \n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=prediction))\n",
    "    tf.summary.scalar('loss',loss)\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) ## argmax 返回一维张量中最大的值所在的位置\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "        \n",
    "## 生成metadata 文件\n",
    "if tf.gfile.Exists(DIR+'projector/projector/metadata.tsv'):\n",
    "    tf.gfile.DeleteRecursively(DIR+'projector/projector/metadata.tsv')\n",
    "with open(DIR+'projector/projector/metadata.tsv','w+') as f:\n",
    "    labels = sess.run(tf.argmax(mnist.test.labels[:],1))\n",
    "    for  i in range(image_num):\n",
    "        f.write(str(labels[i])+'\\n')\n",
    "\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "projector_writer = tf.summary.FileWriter(DIR+'projector/projector',sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "config = projector.ProjectorConfig()\n",
    "embed = config.embeddings.add()\n",
    "embed.tensor_name = embedding.name\n",
    "embed.metadata_path = DIR+'projector/projector/metadata.tsv'\n",
    "embed.sprite.image_path = DIR +'projector/data/mnist_10k_sprite.png'\n",
    "embed.sprite.single_image_dim.extend([28,28])\n",
    "projector.visualize_embeddings(projector_writer,config)\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ## 每个批次 100个 样本\n",
    "    batch_xs,batch_ys = mnist.train.next_batch(100)\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    summary,_=sess.run([merged,train_step],feed_dict={x:batch_xs,y:batch_ys},options=run_options,run_metadata=run_metadata)\n",
    "    projector_writer.add_run_metadata(run_metadata,'step%03d'%i)\n",
    "    projector_writer.add_summary(summary,i)\n",
    "    \n",
    "    if i%100 == 0:  \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print (\"Iter\" + str(i)+\" ,Testing Accuracy\" +str(acc))\n",
    "\n",
    "saver.save(sess,DIR+'projector/projector/a_model.ckpt',global_step=max_steps)\n",
    "projector_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard结果  \n",
    "1.scaler  \n",
    "\n",
    "![5-11](https://github.com/dta0502/DeepLearning-Framework-TensorFlow-Learning-and-Application/blob/master/Notebook%20images/5-11.PNG)\n",
    "\n",
    "![5-12](https://github.com/dta0502/DeepLearning-Framework-TensorFlow-Learning-and-Application/blob/master/Notebook%20images/5-12.PNG)\n",
    "\n",
    "这些都和之前那个差不多\n",
    "\n",
    "2.projector  \n",
    "- 可以选择color，给每个标签不同的颜色   \n",
    "- 刚开始标签都是混乱的，然后点击T-SNE开始训练，可以看出，图片随着迭代次数的增加开始聚类  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六课"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-3-6563f4dcc2e6>:69: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6563f4dcc2e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Iter\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" ,Testing Accuracy\"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "\n",
    "batch_size =100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "##初始化权值\n",
    "def weight_variable(shape):\n",
    "    initial= tf.truncated_normal(shape,stddev=0.1) #生成一个阶段的正太分布\n",
    "    return tf.Variable(initial)\n",
    "## 初始化偏置\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "## 卷积层\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "## 池化层\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "##改变x的格式转变为4D的向量 [batch,in_height,in_weight,in_channel]\n",
    "x_image = tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "##初始化第一个卷积层的权重和偏置\n",
    "W_conv1 = weight_variable([5,5,1,32]) ## 5x5的采样窗口,32个卷积核从1个平面抽取特征\n",
    "b_conv1 = bias_variable([32]) ##每一个卷积核一个偏执量\n",
    "\n",
    "##把x_image 和权值向量进行卷积,再加上偏置量,然后应用于relu 激活函数\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1) ##进行max_pooling\n",
    "\n",
    "## 初始化第二个卷积层的权重和偏置\n",
    "W_conv2 = weight_variable([5,5,32,64]) ## 5x5的采样窗口,64个卷积核从32个平面抽取特征\n",
    "b_conv2 = bias_variable([64]) ##每一个卷积核一个偏执量\n",
    "\n",
    "##把h_pool1 和权值向量进行卷积,再加上偏置量,然后应用于relu 激活函数\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2) ##进行max_pooling\n",
    "\n",
    "## 28x28 的图片第一次 卷积 后还是28x28,第一次池化后变成 14x14\n",
    "## 第二次 卷积后 为 14x14,第二次池化后 成了7x7\n",
    "# 通过上面操作后得到 64张7x7的平面\n",
    "\n",
    "## 初始化第一个全连接层的权值\n",
    "W_fc1 = weight_variable([7*7*64,1024]) # 上一层有了7*7*64个神经元,全连接层有了1024个神经元\n",
    "b_fc1 = bias_variable([1024]) ## 1024个节点\n",
    "\n",
    "## 把吃话曾2的输出扁平化为1维\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "## 求第一个全连接层的输出\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\n",
    "\n",
    "## keep_prob 用来表示 神经元 的输出概率\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "## 初始化第二个全连接层\n",
    "W_fc2 = weight_variable([1024,10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "## 计算输出\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)\n",
    "\n",
    "## 交叉熵代价函数\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "## 使用 AdamOptimizer进行优化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "## 结果存放在一个布尔列表中\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1)) ## argmax返回一维张量中最大的值所在的位置\n",
    "## 求准确率\n",
    "accuracy= tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7})\n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        print (\"Iter\" + str(epoch)+\" ,Testing Accuracy\" +str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第六周作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data \n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "\n",
    "batch_size =100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "##参数概要\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean) # 平均值\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histgram',var) # 直方图\n",
    "        \n",
    "        \n",
    "##初始化权值\n",
    "def weight_variable(shape,name):\n",
    "    initial= tf.truncated_normal(shape,stddev=0.1) #生成一个阶段的正太分布\n",
    "    return tf.Variable(initial,name=name)\n",
    "\n",
    "## 初始化偏置\n",
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial,name=name)\n",
    "\n",
    "## 卷积层\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "## 池化层\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x=tf.placeholder(tf.float32,[None,784],name='x-input')\n",
    "    y = tf.placeholder(tf.float32,[None,10],name='y-input')\n",
    "    with tf.name_scope('x_image'):\n",
    "        x_image = tf.reshape(x,[-1,28,28,1],name='x_image')\n",
    "\n",
    "\n",
    "with tf.name_scope('Conv1'):\n",
    "    with tf.name_scope('W_conv1'):\n",
    "        W_conv1 = weight_variable([5,5,1,32],name='W_conv1')\n",
    "    with tf.name_scope('b_conv1'):\n",
    "        b_conv1 = bias_variable([32],name='b_conv1')\n",
    "    with tf.name_scope('conv2d_1'):\n",
    "        conv2d_1 =conv2d(x_image,W_conv1)+b_conv1\n",
    "    with tf.name_scope('h_conv1'):\n",
    "        h_conv1 = tf.nn.relu(conv2d_1)\n",
    "    with tf.name_scope('h_pool1'):\n",
    "        h_pool1 = max_pool_2x2(h_conv1) \n",
    "\n",
    "\n",
    "with tf.name_scope('Conv2'):\n",
    "    with tf.name_scope('W_conv2'):\n",
    "        W_conv2 = weight_variable([5,5,32,64],name='W_conv2') \n",
    "    with tf.name_scope('b_conv2'):\n",
    "        b_conv2 = bias_variable([64],name='b_conv2') \n",
    "    with tf.name_scope('conv2d_2'):\n",
    "        conv2d_2 =conv2d(h_pool1,W_conv2)+b_conv2\n",
    "    with tf.name_scope('h_conv2'):\n",
    "        h_conv2 = tf.nn.relu(conv2d_2)\n",
    "    with tf.name_scope('h_pool2'):\n",
    "        h_pool2 = max_pool_2x2(h_conv2) \n",
    "        \n",
    "        \n",
    "## 28x28 的图片第一次 卷积 后还是28x28,第一次池化后变成 14x14\n",
    "## 第二次 卷积后 为 14x14,第二次池化后 成了7x7\n",
    "# 通过上面操作后得到 64张7x7的平面\n",
    "\n",
    "with tf.name_scope('fc1'):\n",
    "    with tf.name_scope('W_fc1'):\n",
    "        W_fc1 = weight_variable([7*7*64,1024],name='W_fc1') \n",
    "    with tf.name_scope('b_fc1'):\n",
    "        b_fc1 = bias_variable([1024],name='b_fc1')\n",
    "    with tf.name_scope('h_pool2_flat'):\n",
    "        h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "    with tf.name_scope('wx_plus_b1'):\n",
    "        wx_plus_b1 = tf.matmul(h_pool2_flat,W_fc1)+b_fc1\n",
    "    with tf.name_scope('rule'):\n",
    "        h_fc1 = tf.nn.relu(wx_plus_b1)\n",
    "\n",
    "    with tf.name_scope('keep_prob'):\n",
    "        keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    with tf.name_scope('h_fc1_drop'):\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob,name='h_fc1_drop')\n",
    "\n",
    "\n",
    "with tf.name_scope('fc2'):\n",
    "    with tf.name_scope('W_fc2'):\n",
    "        W_fc2 = weight_variable([1024,10],name='W_fc2')\n",
    "    with tf.name_scope('b_fc2'):\n",
    "        b_fc2 = bias_variable([10],name='b_fc2')\n",
    "    with tf.name_scope('wx_plus_b2'):\n",
    "        wx_plus_b2 = tf.matmul(h_fc1_drop,W_fc2)+b_fc2\n",
    "    with tf.name_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(wx_plus_b2)\n",
    "\n",
    "\n",
    "## 交叉熵代价函数\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "    tf.summary.scalar('cross_entropy',cross_entropy)\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuray'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1)) ## argmax返回一维张量中最大的值所在的位置\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy= tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('logs/train',sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('logs/test',sess.graph)\n",
    "    for i in range(1001):\n",
    "        # 训练模型\n",
    "        batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.5})\n",
    "        ## 记录训练集 记录的参数\n",
    "        summary = sess.run(merged,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "        train_writer.add_summary(summary,i)\n",
    "        ## 记录测量集 计算的参数\n",
    "        batch_xs,batch_ys = mnist.test.next_batch(batch_size)\n",
    "        summary = sess.run(merged,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "        test_writer.add_summary(summary,i)\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "            train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images[:10000],y:mnist.train.labels[:10000],keep_prob:1.0})\n",
    "            print (\"Iter\" + str(i)+\" ,Testing Accuracy\" +str(test_acc)+\" ,Training Accuracy\" +str(train_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第七课\n",
    "循环神经网络（略）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第八课"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saver_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter0 ,Testing Accuracy0.8248\n",
      "Iter1 ,Testing Accuracy0.8916\n",
      "Iter2 ,Testing Accuracy0.9005\n",
      "Iter3 ,Testing Accuracy0.9065\n",
      "Iter4 ,Testing Accuracy0.9082\n",
      "Iter5 ,Testing Accuracy0.9105\n",
      "Iter6 ,Testing Accuracy0.9106\n",
      "Iter7 ,Testing Accuracy0.9125\n",
      "Iter8 ,Testing Accuracy0.9152\n",
      "Iter9 ,Testing Accuracy0.9159\n",
      "Iter10 ,Testing Accuracy0.9167\n",
      "Iter11 ,Testing Accuracy0.9178\n",
      "Iter12 ,Testing Accuracy0.9188\n",
      "Iter13 ,Testing Accuracy0.9189\n",
      "Iter14 ,Testing Accuracy0.9197\n",
      "Iter15 ,Testing Accuracy0.9203\n",
      "Iter16 ,Testing Accuracy0.9208\n",
      "Iter17 ,Testing Accuracy0.9207\n",
      "Iter18 ,Testing Accuracy0.9205\n",
      "Iter19 ,Testing Accuracy0.9213\n",
      "Iter20 ,Testing Accuracy0.9208\n"
     ]
    }
   ],
   "source": [
    "## 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "## 每个批次的大小\n",
    "batch_size =100\n",
    "## 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "## 定义两个placeholder\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "## 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "\n",
    "## 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "## 交叉熵\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = prediction))\n",
    "## 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "## 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "## 结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) ## argmax 返回一维张量中最大的值所在的位置\n",
    "## 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print (\"Iter\" + str(epoch)+\" ,Testing Accuracy\" +str(acc))\n",
    "        \n",
    "    #保存模型\n",
    "    saver.save(sess, 'net/my_net.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saver_restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "0.098\n",
      "INFO:tensorflow:Restoring parameters from net/my_net.ckpt\n",
      "0.9208\n"
     ]
    }
   ],
   "source": [
    "## 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "## 每个批次的大小\n",
    "batch_size =100\n",
    "## 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "## 定义两个placeholder\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "## 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "\n",
    "## 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "## 交叉熵\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = prediction))\n",
    "## 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "## 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "## 结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) ## argmax 返回一维张量中最大的值所在的位置\n",
    "## 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)     \n",
    "    print(sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n",
    "    saver.restore(sess, 'net/my_net.ckpt')\n",
    "    print(sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：运行这段程序的时候，需要restart kernel，如果之前运行过程序，就有可能报错**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载Google图像识别网络inception-v3并查看结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import tarfile\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish: inception-2015-12-05.tgz\n"
     ]
    }
   ],
   "source": [
    "# inception模型下载地址\n",
    "inception_pretrain_model_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "\n",
    "# 模型存放地址\n",
    "inception_pretrain_model_dir = \"inception_model\"\n",
    "if not os.path.exists(inception_pretrain_model_dir):\n",
    "    os.makedirs(inception_pretrain_model_dir)\n",
    "    \n",
    "# 获取文件名以及文件路径\n",
    "filename = inception_pretrain_model_url.split('/')[-1]\n",
    "filepath = os.path.join(inception_pretrain_model_dir, filename)\n",
    "\n",
    "# 下载模型\n",
    "if not os.path.exists(filepath):\n",
    "    print(\"download:\", filename)\n",
    "    r = requests.get(inception_pretrain_model_url, stream = True)\n",
    "    with open(filepath, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size = 1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "print(\"finish:\", filename)\n",
    "\n",
    "# 解压文件\n",
    "tarfile.open(filepath, \"r:gz\").extractall(inception_pretrain_model_dir)\n",
    "\n",
    "# 模型结构存放文件\n",
    "log_dir = 'inception_log'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# classify_image_graph_def.pb为google训练好的模型\n",
    "inception_graph_def_file = os.path.join(inception_pretrain_model_dir, 'classify_image_graph_def.pb')\n",
    "with tf.Session() as sess:\n",
    "    # 创建一个图用来存放google训练好的模型\n",
    "    with tf.gfile.FastGFile(inception_graph_def_file, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        tf.import_graph_def(graph_def, name = '')\n",
    "        \n",
    "    # 保存图的结构\n",
    "    writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用inception-v3做各种图像识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imagenet_2012_challenge_label_map_proto.pbtxt\n",
    "- target_class是指1000分类的类别编号  \n",
    "- target_class_string是一个字符串编号，可以从human_label_map.txt中找到对应的编号，便可以看到这张图片的内容  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 40\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "import os  \n",
    "import numpy as np  \n",
    "import re  \n",
    "from PIL import Image  \n",
    "import matplotlib.pyplot as plt  \n",
    "  \n",
    "class NodeLookup(object):  \n",
    "    def __init__(self):  \n",
    "        label_lookup_path = 'inception_model/imagenet_2012_challenge_label_map_proto.pbtxt'  \n",
    "        uid_lookup_path = 'inception_model/imagenet_synset_to_human_label_map.txt'  \n",
    "        self.node_lookup = self.load(label_lookup_path, uid_lookup_path)  \n",
    "  \n",
    "    def load(self, label_lookup_path, uid_lookup_path):  \n",
    "        #加载分类字符串n ------ 对应分类名称的文件  \n",
    "        proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()  \n",
    "        uid_to_human = {}  \n",
    "        #一行一行读取数据  \n",
    "        for line in proto_as_ascii_lines :  \n",
    "            #去掉换行符  \n",
    "            line = line.strip('\\n')  \n",
    "            #按照‘\\t’分割  \n",
    "            parsed_items = line.split('\\t')  \n",
    "            #获取分类编号和分类名称  \n",
    "            uid = parsed_items[0]  \n",
    "            human_string = parsed_items[1]  \n",
    "            #保存编号字符串-----与分类名称映射关系  \n",
    "            uid_to_human[uid] = human_string  \n",
    "  \n",
    "  \n",
    "        #加载分类字符串n ----- 对应分类编号1-1000的文件  \n",
    "        proto_as_ascii_lines = tf.gfile.GFile(label_lookup_path).readlines()  \n",
    "        node_id_to_uid = {}  \n",
    "        for line in proto_as_ascii_lines :  \n",
    "            if line.startswith('  target_class:'):  \n",
    "                #获取分类编号1-1000  \n",
    "                target_class = int(line.split(': ')[1])  \n",
    "            if line.startswith('  target_class_string:'):  \n",
    "                #获取编号字符串n****  \n",
    "                target_class_string = line.split(': ')[1]  \n",
    "                #保存分类编号1-1000与编号字符串n****的映射关系  \n",
    "                node_id_to_uid[target_class] = target_class_string[1:-2]  \n",
    "  \n",
    "  \n",
    "        #建立分类编号1-1000对应分类名称的映射关系  \n",
    "        node_id_to_name = {}  \n",
    "        for key, val in node_id_to_uid.items():  \n",
    "            #获取分类名称  \n",
    "            name = uid_to_human[val]  \n",
    "            #建立分类编号1-1000到分类名称的映射关系  \n",
    "            node_id_to_name[key] = name  \n",
    "        return node_id_to_name  \n",
    "  \n",
    "    #传入分类编号1-1000返回分类名称  \n",
    "    def id_to_string(self, node_id):  \n",
    "        if node_id not in self.node_lookup:  \n",
    "            return ''  \n",
    "        return self.node_lookup[node_id]  \n",
    "\n",
    "# 创建一个图来存放google训练好的模型  #2 load graph  \n",
    "with tf.gfile.FastGFile('inception_model/classify_image_graph_def.pb', 'rb') as f:  \n",
    "    graph_def = tf.GraphDef()  \n",
    "    graph_def.ParseFromString(f.read())  \n",
    "    tf.import_graph_def(graph_def, name='')  \n",
    "  \n",
    "  \n",
    "with tf.Session() as sess:  \n",
    "    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')  \n",
    "    #遍历目录  \n",
    "    for root, dirs, files in os.walk('images/'):  \n",
    "        for file in files:  \n",
    "            #载入图片  \n",
    "            image_data = tf.gfile.FastGFile(os.path.join(root,file), 'rb').read()  \n",
    "            predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})#图片格式是jpg格式  \n",
    "            predictions = np.squeeze(predictions)#把结果转为1维  \n",
    "  \n",
    "            #打印图片路径及名称  \n",
    "            image_path = os.path.join(root,file)  \n",
    "            print(image_path)  \n",
    "            #显示图片  \n",
    "            img = Image.open(image_path)  \n",
    "            plt.imshow(img)  \n",
    "            plt.axis('off')  \n",
    "            plt.show()  \n",
    "  \n",
    "            #排序  \n",
    "            top_k = predictions.argsort()[-5:][::-1]  \n",
    "            node_lookup = NodeLookup()  \n",
    "            for node_id in top_k:  \n",
    "                #获取分类名称  \n",
    "                human_string = node_lookup.id_to_string(node_id)  \n",
    "                #获取该分类的置信度  \n",
    "                score = predictions[node_id]  \n",
    "                print('%s (score = %.5f)' % (human_string, score))  \n",
    "            print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第九课\n",
    "\n",
    "### TensorFlow-GPU\n",
    "- cuda的安装  \n",
    "- TensorFlow的GPU版本的安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第十课\n",
    "略"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
